#!/usr/bin/env python3
"""
å°çº¢ä¹¦è‡ªåŠ¨åŒ–å·¥å…· - ä¸»ç¨‹åº
æ•´åˆæœç´¢ã€AIåˆ†æã€å›¾ç‰‡ç”Ÿæˆã€å‘å¸ƒå…¨æµç¨‹

ä½¿ç”¨æ–¹æ³•:
    python main.py --keyword "æ˜¥æ—¥ç©¿æ­" --max-notes 50 --topics 10
"""

import argparse
import json
import os
import sys
import time
import yaml
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from modules.search import XHSAdvancedSearch, DataQualityFilter
from modules.ai_engine import AIEngine
from modules.image_gen import ImageGenerator
from modules.publisher import XHSPublisher


def safe_print(text: str, end: str = "\n", flush: bool = True):
    """å®‰å…¨æ‰“å°å‡½æ•°ï¼Œç¡®ä¿è¾“å‡ºè¢«æ­£ç¡®æ˜¾ç¤º"""
    print(text, end=end, flush=flush)


class XHSAutomation:
    """å°çº¢ä¹¦è‡ªåŠ¨åŒ–ä¸»æ§ç±»"""

    def __init__(self, config_path: str = "config/config.yaml"):
        """
        åˆå§‹åŒ–è‡ªåŠ¨åŒ–å·¥å…·

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å– API Keyï¼Œå…¶æ¬¡ä»é…ç½®æ–‡ä»¶è¯»å–
        self.api_key = os.environ.get("DASHSCOPE_API_KEY") or os.environ.get("ALIYUN_API_KEY") or self.config.get("aliyun", {}).get("api_key", "")

        if not self.api_key or self.api_key == "your-dashscope-api-key-here":
            raise ValueError(
                "è¯·å…ˆé…ç½®é˜¿é‡Œäº‘ç™¾ç‚¼ API Keyï¼\n"
                "æ–¹æ³•1: è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY æˆ– ALIYUN_API_KEY\n"
                "æ–¹æ³•2: åœ¨ config/config.yaml ä¸­å¡«å†™ api_key\n"
                "æç¤º: è¯·å‹¿å°†çœŸå® API Key æäº¤åˆ°ä»£ç ä»“åº“"
            )

    def _load_config(self, path: str) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            print("ä½¿ç”¨é»˜è®¤é…ç½®...")
            return self._default_config()
        except Exception as e:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            return self._default_config()

    def _default_config(self) -> dict:
        """é»˜è®¤é…ç½®"""
        return {
            "aliyun": {"api_key": ""},
            "search": {
                "default_sort": "time_descending",
                "default_note_type": 51,
                "max_notes": 50,
                "min_likes": 0,
            },
            "content": {
                "topic_count": 10,
                "images_per_topic": 5,
                "image_size": "768*1152",
            },
            "publish": {
                "min_interval": 120,
                "max_interval": 180,
                "manual_confirm": True,
            },
        }

    def _create_session_dir(self, keyword: str, timestamp: str) -> Path:
        """
        åˆ›å»ºä¼šè¯ç›®å½•ï¼ˆkeyword + æ—¶é—´æˆ³ï¼‰

        Args:
            keyword: å…³é”®è¯
            timestamp: æ—¶é—´æˆ³

        Returns:
            ä¼šè¯ç›®å½•è·¯å¾„
        """
        # æ¸…ç†å…³é”®è¯ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
        safe_keyword = "".join(
            c for c in keyword if c.isalnum() or c in (" ", "_", "-")
        ).strip()
        safe_keyword = safe_keyword[:20] if len(safe_keyword) > 20 else safe_keyword

        # åˆ›å»ºç›®å½•ï¼šresults/{keyword}_{timestamp}/
        session_dir = Path("results") / f"{safe_keyword}_{timestamp}"
        session_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºå­ç›®å½•
        (session_dir / "images").mkdir(exist_ok=True)
        (session_dir / "data").mkdir(exist_ok=True)

        return session_dir

    def run(
        self,
        keyword: str,
        max_notes: int = 50,
        topic_count: int = 10,
        min_likes: int = 0,
        skip_search: bool = False,
        skip_ai: bool = False,
        skip_image: bool = False,
        skip_publish: bool = False,
        notes_file: str = "",
        topics_file: str = "",
        debug: bool = False,
        auto_publish: bool = False,
        verbose: bool = False,
        ai_model: str = "qwen-plus",
        enable_thinking: bool = False,
    ):
        """
        è¿è¡Œå®Œæ•´æµç¨‹

        Args:
            keyword: æœç´¢å…³é”®è¯
            max_notes: æœ€å¤§è·å–ç¬”è®°æ•°
            topic_count: ç”Ÿæˆè¯é¢˜æ•°é‡
            min_likes: æœ€å°ç‚¹èµæ•°è¿‡æ»¤ï¼ˆ0è¡¨ç¤ºä¸è¿‡æ»¤ï¼‰
            skip_search: è·³è¿‡æœç´¢
            skip_ai: è·³è¿‡AIåˆ†æ
            skip_image: è·³è¿‡å›¾ç‰‡ç”Ÿæˆ
            skip_publish: è·³è¿‡å‘å¸ƒ
            notes_file: ä»æ–‡ä»¶åŠ è½½ç¬”è®°
            topics_file: ä»æ–‡ä»¶åŠ è½½è¯é¢˜
            debug: å¼€å¯è°ƒè¯•æ¨¡å¼
            auto_publish: è‡ªåŠ¨å‘å¸ƒï¼ˆæ— äººå·¥ç¡®è®¤ï¼Œå¯èƒ½è§¦å‘é£æ§ï¼‰
            verbose: è¯¦ç»†è¾“å‡ºæ¨¡å¼ï¼ˆæ˜¾ç¤ºå®Œæ•´çš„APIè¾“å…¥è¾“å‡ºï¼‰
            ai_model: AIæ¨¡å‹é€‰æ‹©ï¼ˆqwen-plus/qwen-max/qwen-turboï¼‰
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆä¼šæ˜¾è‘—å¢åŠ å“åº”æ—¶é—´ï¼‰
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä½¿ç”¨é…ç½®æˆ–å‚æ•°
        max_notes = max_notes or self.config["search"].get("max_notes", 50)
        topic_count = topic_count or self.config["content"].get("topic_count", 10)
        min_likes = (
            min_likes if min_likes >= 0 else self.config["search"].get("min_likes", 0)
        )

        # åˆ›å»ºä¼šè¯ç›®å½•
        session_dir = self._create_session_dir(keyword, timestamp)
        data_dir = session_dir / "data"
        images_dir = session_dir / "images"
        
        # éªŒè¯ç›®å½•æ˜¯å¦åˆ›å»ºæˆåŠŸ
        if not session_dir.exists():
            print(f"âŒ æ— æ³•åˆ›å»ºä¼šè¯ç›®å½•: {session_dir}")
            return
        if not data_dir.exists():
            print(f"âŒ æ— æ³•åˆ›å»ºæ•°æ®ç›®å½•: {data_dir}")
            return
        if not images_dir.exists():
            print(f"âŒ æ— æ³•åˆ›å»ºå›¾ç‰‡ç›®å½•: {images_dir}")
            return

        print("\n" + "=" * 70)
        print("ğŸš€ å°çº¢ä¹¦è‡ªåŠ¨åŒ–å·¥å…·å¯åŠ¨")
        print("=" * 70)
        print(f"ğŸ“Œ å…³é”®è¯: {keyword}")
        print(f"ğŸ“Š çˆ¬å–ç¬”è®°æ•°: {max_notes} æ¡")
        print(f"âœ¨ AI æå–è¯é¢˜æ•°: {topic_count} ä¸ª")
        print(f"ğŸ‘ æœ€å°ç‚¹èµæ•°: {min_likes}")
        print(f"ğŸ”§ è°ƒè¯•æ¨¡å¼: {'å¼€å¯' if debug else 'å…³é—­'}")
        print(f"ğŸ“¢ è¯¦ç»†è¾“å‡º: {'å¼€å¯' if verbose else 'å…³é—­'}")
        print(f"ğŸ¤– AIæ¨¡å‹: {ai_model}")
        print(f"ğŸ’­ æ€è€ƒæ¨¡å¼: {'å¼€å¯ï¼ˆå“åº”è¾ƒæ…¢ï¼‰' if enable_thinking else 'å…³é—­ï¼ˆæ¨èï¼‰'}")
        print(
            f"ğŸ”˜ å‘å¸ƒæ¨¡å¼: {'å…¨è‡ªåŠ¨ï¼ˆé«˜é£é™©ï¼‰' if auto_publish else 'äººå·¥ç¡®è®¤ï¼ˆæ¨èï¼‰'}"
        )
        print(f"ğŸ“ å·¥ä½œç›®å½•: {session_dir}")
        print(f"â° æ—¶é—´æˆ³: {timestamp}")
        print("=" * 70 + "\n")

        notes = []
        ai_result = {}
        topics = []

        # ========== Step 1: é«˜çº§æœç´¢ ==========
        if not skip_search and notes_file == "":
            safe_print("\n" + "#"*70)
            safe_print("ğŸ“¥ STEP 1: æœç´¢ç¬”è®°")
            safe_print("#"*70 + "\n", flush=True)

            with XHSAdvancedSearch() as searcher:
                notes = searcher.search_with_filter(
                    keyword=keyword,
                    sort=self.config["search"].get("default_sort", "time_descending"),
                    note_type=self.config["search"].get("default_note_type", 51),
                    max_notes=max_notes,
                    min_likes=min_likes,
                    debug=debug,
                )

            # æ•°æ®è´¨é‡äºŒæ¬¡ç­›é€‰ï¼ˆä»…åœ¨æœ‰è¶³å¤Ÿæ•°æ®æ—¶ï¼‰
            if len(notes) > 20:
                print("\nğŸ” æ•°æ®è´¨é‡ç­›é€‰...")
                quality_filter = DataQualityFilter()
                notes = quality_filter.get_top_notes(notes, n=min(50, len(notes)))

            # ä¿å­˜æœç´¢ç»“æœåˆ°ä¼šè¯ç›®å½•
            if notes:
                notes_path = data_dir / "notes.json"
                print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æœç´¢ç»“æœ...")
                print(f"   æ–‡ä»¶è·¯å¾„: {notes_path}")
                print(f"   ç¬”è®°æ•°é‡: {len(notes)}")
                
                try:
                    with open(notes_path, "w", encoding="utf-8") as f:
                        json.dump(notes, f, ensure_ascii=False, indent=2)
                    
                    # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„ä¿å­˜äº†
                    if notes_path.exists():
                        file_size = notes_path.stat().st_size / 1024
                        print(f"   âœ… ä¿å­˜æˆåŠŸ: {file_size:.1f} KB")
                    else:
                        print(f"   âŒ ä¿å­˜å¤±è´¥: æ–‡ä»¶ä¸å­˜åœ¨")
                except Exception as e:
                    print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()

        elif notes_file != "":
            print(f"\nğŸ“‚ ä»æ–‡ä»¶åŠ è½½ç¬”è®°: {notes_file}")
            with open(notes_file, "r", encoding="utf-8") as f:
                notes = json.load(f)
            print(f"âœ… å·²åŠ è½½ {len(notes)} æ¡ç¬”è®°")
        else:
            print("â­ï¸ è·³è¿‡æœç´¢æ­¥éª¤")

        # åªæœ‰åœ¨éœ€è¦AIåˆ†ææ—¶æ‰æ£€æŸ¥notes
        if not skip_ai and topics_file == "":
            if not notes:
                print("âŒ æ²¡æœ‰å¯ç”¨çš„ç¬”è®°æ•°æ®ï¼Œé€€å‡º")
                return

        # ========== Step 2: AI åˆ†æ ==========
        if not skip_ai and topics_file == "":
            safe_print("\n" + "#"*70)
            safe_print("ğŸ¤– STEP 2: AI çƒ­ç‚¹åˆ†æ")
            safe_print("#"*70 + "\n", flush=True)

            ai = AIEngine(api_key=self.api_key, model=ai_model, enable_thinking=enable_thinking)
            
            # åˆ›å»ºè‡ªå®šä¹‰æ—¥å¿—å›è°ƒï¼Œæ”¯æŒè¯¦ç»†è¾“å‡º
            def verbose_log(msg):
                safe_print(msg, flush=True)
            
            if verbose:
                safe_print("=" * 70)
                safe_print("ğŸ“‹ è¯¦ç»†è¾“å‡ºæ¨¡å¼å·²å¼€å¯ï¼Œå°†æ˜¾ç¤ºå®Œæ•´çš„APIäº¤äº’å†…å®¹")
                safe_print("=" * 70 + "\n")
            
            ai_result = ai.analyze_and_create_topics(
                notes=notes, 
                keyword=keyword, 
                top_n=topic_count,
                log_callback=verbose_log
            )
            topics = ai_result["topics"]

            # ä¿å­˜åˆ†æç»“æœåˆ°ä¼šè¯ç›®å½•
            ai_path = data_dir / "ai_result.json"
            print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜AIåˆ†æç»“æœ...")
            print(f"   æ–‡ä»¶è·¯å¾„: {ai_path}")
            
            try:
                with open(ai_path, "w", encoding="utf-8") as f:
                    json.dump(ai_result, f, ensure_ascii=False, indent=2)
                
                # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„ä¿å­˜äº†
                if ai_path.exists():
                    file_size = ai_path.stat().st_size / 1024
                    print(f"   âœ… ä¿å­˜æˆåŠŸ: {file_size:.1f} KB")
                else:
                    print(f"   âŒ ä¿å­˜å¤±è´¥: æ–‡ä»¶ä¸å­˜åœ¨")
            except Exception as e:
                print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

            # æ‰“å°åˆ†ææ‘˜è¦
            print("\nğŸ“Š åˆ†ææ‘˜è¦:")
            analyze = ai_result["analyze_result"]
            keywords = analyze.get('top_keywords', [])[:5]
            emotions = analyze.get('emotion_points', [])[:3]
            if keywords:
                print(f"   å…³é”®è¯: {', '.join(keywords)}")
            if emotions:
                print(f"   æƒ…ç»ªç‚¹: {', '.join(emotions)}")

        elif topics_file != "":
            print(f"\nğŸ“‚ ä»æ–‡ä»¶åŠ è½½è¯é¢˜: {topics_file}")
            with open(topics_file, "r", encoding="utf-8") as f:
                if topics_file.endswith(".json"):
                    data = json.load(f)
                    if "topics" in data:
                        topics = data["topics"]
                    else:
                        topics = data
                else:
                    topics = json.load(f)
            print(f"âœ… å·²åŠ è½½ {len(topics)} ä¸ªè¯é¢˜")
        else:
            print("â­ï¸ è·³è¿‡AIåˆ†ææ­¥éª¤")

        if not topics:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„è¯é¢˜æ•°æ®ï¼Œé€€å‡º")
            return

        # ========== Step 3: æ–‡ç”Ÿå›¾ ==========
        if not skip_image:
            safe_print("\n" + "#"*70)
            safe_print("ğŸ–¼ï¸ STEP 3: ç”Ÿæˆé…å›¾")
            safe_print("#"*70 + "\n", flush=True)

            img_gen = ImageGenerator(
                api_key=self.api_key,
                save_dir=str(images_dir),  # ä½¿ç”¨ä¼šè¯ç›®å½•ä¸‹çš„ images æ–‡ä»¶å¤¹
            )
            
            # åˆ›å»ºè‡ªå®šä¹‰æ—¥å¿—å›è°ƒ
            def verbose_log(msg):
                safe_print(msg, flush=True)

            topics = img_gen.generate_for_topics(
                topics=topics,
                n_per_topic=self.config["content"].get("images_per_topic", 5),
                size=self.config["content"].get("image_size", "768*1152"),
                log_callback=verbose_log
            )

            # ä¿å­˜å¸¦å›¾ç‰‡è·¯å¾„çš„è¯é¢˜åˆ°ä¼šè¯ç›®å½•
            topics_img_path = data_dir / "topics_with_images.json"
            print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜è¯é¢˜æ•°æ®...")
            print(f"   æ–‡ä»¶è·¯å¾„: {topics_img_path}")
            print(f"   è¯é¢˜æ•°é‡: {len(topics)}")
            
            try:
                with open(topics_img_path, "w", encoding="utf-8") as f:
                    json.dump(topics, f, ensure_ascii=False, indent=2)
                
                # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„ä¿å­˜äº†
                if topics_img_path.exists():
                    file_size = topics_img_path.stat().st_size / 1024
                    print(f"   âœ… ä¿å­˜æˆåŠŸ: {file_size:.1f} KB")
                else:
                    print(f"   âŒ ä¿å­˜å¤±è´¥: æ–‡ä»¶ä¸å­˜åœ¨")
            except Exception as e:
                print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

        else:
            print("â­ï¸ è·³è¿‡å›¾ç‰‡ç”Ÿæˆæ­¥éª¤")

        # ========== Step 4: å‘å¸ƒ ==========
        if not skip_publish:
            safe_print("\n" + "#"*70)
            safe_print("ğŸ“¤ STEP 4: å‘å¸ƒç¬”è®°")
            safe_print("#"*70 + "\n", flush=True)

            # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡
            valid_topics = [t for t in topics if t.get("image_paths")]
            if not valid_topics:
                print("âš ï¸ æ²¡æœ‰å¸¦å›¾ç‰‡çš„è¯é¢˜ï¼Œè·³è¿‡å‘å¸ƒ")
                return

            print(f"ğŸ“‹ å‡†å¤‡å‘å¸ƒ {len(valid_topics)} ç¯‡ç¬”è®°\n")

            with XHSPublisher() as publisher:
                results = publisher.publish_batch(
                    topics=valid_topics,
                    min_interval=self.config["publish"].get("min_interval", 120),
                    max_interval=self.config["publish"].get("max_interval", 180),
                    manual_confirm=not auto_publish,
                )

            # ä¿å­˜å‘å¸ƒç»“æœåˆ°ä¼šè¯ç›®å½•
            publish_result = {
                "timestamp": timestamp,
                "keyword": keyword,
                "total": len(valid_topics),
                "success": sum(results),
                "failed": len(results) - sum(results),
                "topics": [
                    {**t, "published": r} for t, r in zip(valid_topics, results)
                ],
            }
            publish_path = data_dir / "publish_result.json"
            print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜å‘å¸ƒç»“æœ...")
            print(f"   æ–‡ä»¶è·¯å¾„: {publish_path}")
            
            try:
                with open(publish_path, "w", encoding="utf-8") as f:
                    json.dump(publish_result, f, ensure_ascii=False, indent=2)
                
                # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„ä¿å­˜äº†
                if publish_path.exists():
                    file_size = publish_path.stat().st_size / 1024
                    print(f"   âœ… ä¿å­˜æˆåŠŸ: {file_size:.1f} KB")
                else:
                    print(f"   âŒ ä¿å­˜å¤±è´¥: æ–‡ä»¶ä¸å­˜åœ¨")
            except Exception as e:
                print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
            
            print(f"\nğŸ’¾ å‘å¸ƒç»“æœå·²ä¿å­˜: {publish_path}")

        else:
            print("â­ï¸ è·³è¿‡å‘å¸ƒæ­¥éª¤")

        print("\n" + "=" * 70)
        print(f"âœ… æµç¨‹æ‰§è¡Œå®Œæ¯•ï¼æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {session_dir}")
        print("=" * 70 + "\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="å°çº¢ä¹¦è‡ªåŠ¨åŒ–å·¥å…· - æœç´¢â†’åˆ†æâ†’ç”Ÿæˆâ†’å‘å¸ƒ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å®Œæ•´æµç¨‹ï¼ˆæ¨èï¼Œäººå·¥ç¡®è®¤å‘å¸ƒï¼‰
  python main.py --keyword "æ˜¥æ—¥ç©¿æ­"

  # è‡ªå®šä¹‰çˆ¬å–æ•°é‡å’Œè¯é¢˜æ•°é‡
  python main.py --keyword "æ˜¥æ—¥ç©¿æ­" --max-notes 100 --topics 15
  python main.py --keyword "æ˜¥æ—¥ç©¿æ­" -n 100 -t 15  # ç®€å†™å½¢å¼

  # å¿«é€Ÿæµ‹è¯•ï¼ˆå°‘é‡æ•°æ®ï¼‰
  python main.py --keyword "æ˜¥æ—¥ç©¿æ­" -n 20 -t 3

  # ä½¿ç”¨å¿«é€Ÿæ¨¡å‹ï¼ˆæ¨èï¼Œå“åº”æ›´å¿«ï¼‰
  python main.py --keyword "æ˜¥æ—¥ç©¿æ­" --ai-model qwen-turbo

  # å…¨è‡ªåŠ¨å‘å¸ƒï¼ˆâš ï¸ é«˜é£é™©ï¼Œå¯èƒ½è§¦å‘é£æ§ï¼‰
  python main.py --keyword "æ˜¥æ—¥ç©¿æ­" --auto-publish

  # è°ƒè¯•æ¨¡å¼ï¼ˆæŸ¥çœ‹æ•°æ®ç»“æ„ï¼‰
  python main.py --keyword "æ˜¥æ—¥ç©¿æ­" --debug

  # è¯¦ç»†è¾“å‡ºæ¨¡å¼ï¼ˆæŸ¥çœ‹å®Œæ•´APIäº¤äº’ï¼‰
  python main.py --keyword "æ˜¥æ—¥ç©¿æ­" --verbose

  # ä¸é™åˆ¶ç‚¹èµæ•°ï¼ˆè·å–æ›´å¤šç¬”è®°ï¼‰
  python main.py --keyword "æ˜¥æ—¥ç©¿æ­" --min-likes 0

  # ä»…æœç´¢
  python main.py --keyword "ç¾å¦†" --skip-ai --skip-image --skip-publish

  # ä»å·²æœ‰å†…å®¹ç›´æ¥å‘å¸ƒ
  python main.py --keyword "é©¬å¹´æ˜¥èŠ‚" --skip-search --skip-ai --topics-file results/é©¬å¹´æ˜¥èŠ‚_xxx/data/topics_with_images.json

  # æŸ¥çœ‹å¸®åŠ©
  python main.py --help
        """,
    )

    parser.add_argument("--keyword", "-k", required=True, help="æœç´¢å…³é”®è¯")
    parser.add_argument(
        "--max-notes", 
        "-n", 
        type=int, 
        default=50,
        help="çˆ¬å–ç¬”è®°æ•°é‡ (é»˜è®¤: 50ï¼Œå»ºè®® 20-100)"
    )
    parser.add_argument(
        "--topics", 
        "-t", 
        type=int, 
        default=10,
        help="AI æ™ºèƒ½æå–çš„è¯é¢˜æ•°é‡ (é»˜è®¤: 10ï¼Œå»ºè®® 5-20)"
    )
    parser.add_argument(
        "--min-likes",
        "-l",
        type=int,
        default=0,
        help="æœ€å°ç‚¹èµæ•°è¿‡æ»¤ï¼Œ0è¡¨ç¤ºä¸è¿‡æ»¤ (é»˜è®¤: 0)",
    )
    parser.add_argument(
        "--config", "-c", default="config/config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--debug", "-d", action="store_true", help="å¼€å¯è°ƒè¯•æ¨¡å¼ï¼Œè¾“å‡ºè¯¦ç»†æ•°æ®ç»“æ„"
    )
    parser.add_argument(
        "--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡ºæ¨¡å¼ï¼Œæ˜¾ç¤ºå®Œæ•´çš„APIè¾“å…¥è¾“å‡ºå†…å®¹"
    )
    
    parser.add_argument(
        "--ai-model", 
        default="qwen-plus", 
        choices=["qwen-plus", "qwen-max", "qwen-turbo", "qwen3-max-2026-01-23"],
        help="AIæ¨¡å‹é€‰æ‹© (é»˜è®¤: qwen-plusï¼Œæ¨èä½¿ç”¨qwen-plusæˆ–qwen-turboä»¥è·å¾—æ›´å¿«å“åº”)"
    )
    parser.add_argument(
        "--enable-thinking",
        action="store_true",
        help="å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆä»…æ”¯æŒqwen3ç³»åˆ—ï¼Œä¼šæ˜¾è‘—å¢åŠ å“åº”æ—¶é—´ï¼Œä¸æ¨èï¼‰"
    )

    parser.add_argument("--skip-search", action="store_true", help="è·³è¿‡æœç´¢æ­¥éª¤")
    parser.add_argument("--skip-ai", action="store_true", help="è·³è¿‡AIåˆ†ææ­¥éª¤")
    parser.add_argument("--skip-image", action="store_true", help="è·³è¿‡å›¾ç‰‡ç”Ÿæˆæ­¥éª¤")
    parser.add_argument("--skip-publish", action="store_true", help="è·³è¿‡å‘å¸ƒæ­¥éª¤")

    parser.add_argument("--notes-file", help="ä»JSONæ–‡ä»¶åŠ è½½ç¬”è®°æ•°æ®")
    parser.add_argument("--topics-file", help="ä»JSONæ–‡ä»¶åŠ è½½è¯é¢˜æ•°æ®")
    parser.add_argument(
        "--auto-publish",
        action="store_true",
        help="è‡ªåŠ¨å‘å¸ƒï¼ˆæ— äººå·¥ç¡®è®¤ï¼Œâš ï¸é«˜é£é™©ï¼šå¯èƒ½è§¦å‘å¹³å°é£æ§ï¼‰",
    )

    args = parser.parse_args()

    try:
        app = XHSAutomation(config_path=args.config)
        app.run(
            keyword=args.keyword,
            max_notes=args.max_notes or 50,
            topic_count=args.topics or 10,
            min_likes=args.min_likes,
            skip_search=args.skip_search,
            skip_ai=args.skip_ai,
            skip_image=args.skip_image,
            skip_publish=args.skip_publish,
            notes_file=args.notes_file or "",
            topics_file=args.topics_file or "",
            debug=args.debug,
            auto_publish=args.auto_publish,
            verbose=args.verbose,
            ai_model=args.ai_model,
            enable_thinking=args.enable_thinking,
        )
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ æ‰§è¡Œå‡ºé”™: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
